import openai
import time
import tweepy
import os
import re
import json

from tweet_score import find_score

openai.api_key = "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"

consumer_key = "MSd2CjX2z3D7y8mTTPSQg3CsV"
consumer_secret = "5CFBQgrgWN2Fk9dr9vpYNyyHFTc8J1aiirtqnBPCiPseSgi0eJ"
access_token = "1304455384304762880-Ill9A1L1zXgurjH1Ap4bxLIAAW7Ds2"
access_token_secret = "5Bz93vrdl4tGjhe7LnaRRPPx6YaX2qLDEt3CaFImCkJLU"

# Authenticate to Twitter
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
twitter_api = tweepy.API(auth, wait_on_rate_limit=True)

# loading the news.json dataset

# dtset = pd.read_json(os.path.join(os.path.dirname(__file__), "./data/news.json"))
# queries = dtset.loc[:, "title"]
# with open(os.path.join(os.path.dirname(__file__), "./data/news.json")) as f:
with open(os.path.join(os.path.dirname(__file__),"./data/news.json")) as f:
    dtset = json.load(f)

# print(len(dtset))

# looping through each article
final_data = []
pattern = re.compile(r"\d+\.\s*(.*)")  # matches one or more digits followed by a period, optionally followed by
# whitespace (the \s*), and then captures any character except a newline in a group (the (.*)).
n=10
for i in range(0,50,n):
    batch = dtset[i:i+n]
    for ix in range(len(batch)):
        prompt = """Give 2 most relevant topics based on
        following news headline : {}""".format(
            dtset[ix]["title"]
        )

        while True:
            try:
                response = openai.Completion.create(engine="text-davinci-003", prompt=prompt, max_tokens=256)
                break
            except openai.Error as e:
                print(f"OpenAI error: {e}")
                time.sleep(10)

        # preprocessing responses generated by davinci model

        query_response = response["choices"][0]["text"]
        # print(query_response)

        query_array = re.findall(pattern, query_response.strip())
        # print(query_array)

        query_string = " OR ".join(f'"{term}"' for term in query_array)

        query_string_noRetweets = f"{query_string} -filter:retweets -filter:replies"

        # print(query_string_noRetweets)
        # query_expression = ast.parse(query_string, mode="eval").body
        # print(query_expression)

        # recent_tweets = twitter_api.search_tweets(q=query_string_noRetweets, count=3)

        while True:
            try:
                recent_tweets = tweepy.Cursor(
                    twitter_api.search_tweets, q=query_string_noRetweets, lang="en",tweet_mode="extended",include_entities=True
                ).items(
                    9
                )  # get 9 tweets per article
                break
            except tweepy.TweepError as e:
                print(f"Tweepy error: {e}")
                time.sleep(10)

        # print(recent_tweets)

        tweet_json_data = {"title_of_article": dtset[ix]["title"], "search_query": query_string, "tweets": []}

        for tweet in recent_tweets:
            image_url = []
            video_info = {}
            url_of_tweet = "https://twitter.com/twitter/statuses/" + tweet.id_str
            if "media" in tweet.entities:
                for image in tweet.entities["media"]:
                    image_url.append(image["media_url"])
            if "extended_entities" in tweet._json:
                video_info = tweet.extended_entities["media"][0].get("video_info")
            if tweet.full_text and url_of_tweet:
                user = {
                    "name": tweet.user.name,
                    "screen_name": tweet.user.screen_name,
                    "profile_image_url": tweet.user.profile_image_url,
                    "location": tweet.user.location,
                    "description": tweet.user.description,
                    "friends": tweet.user.friends_count,
                    "followers": tweet.user.followers_count,
                }
                retweets = tweet.retweet_count
                favourites = tweet.favorite_count
                created_at = tweet.created_at
                hashtags = tweet.entities["hashtags"]
                hash_text=[]
                if hashtags:
                    for g in range(len(hashtags)):
                        hash_text.append(hashtags[g]['text'])
                score = find_score(
                    user["description"],
                    user["followers"],
                    created_at,
                    tweet.full_text,
                    favourites,
                    retweets,
                    hash_text,
                    dtset[ix]["title"],
                    dtset[ix]["text"],
                    dtset[ix]["date"],
                )
                tweet_json_data["tweets"].append(
                    {
                        "tweet_id": tweet.id,
                        "tweet_created_at": str(created_at),
                        "tweet_text": tweet.full_text,
                        "hashtags": hash_text,
                        "tweet_url": url_of_tweet,
                        "tweet_images_url": image_url,
                        "tweet_video_info": video_info,
                        "user_info": user,
                        "tweet_retweetcount": retweets,
                        "tweet_likecount": favourites,
                        "tweet_score": score,
                    }
                )

        final_data.append(tweet_json_data)
        time.sleep(2)


# writing the tweets into a json file
with open(os.path.join(os.path.dirname(__file__),"./data/tweets.json"), "w") as json_file:
    json.dump(final_data, json_file)

